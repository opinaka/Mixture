{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP03Iy4GdxVWi2qhu7CMZz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opinaka/Mixture/blob/master/spark2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "H0FWUS_ztn8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4A3V2Z6thxX",
        "outputId": "38958b45-6438-42e4-f850-c0e200074212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temps de téléchargement : 16.685839653015137 secondes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "end_time = time.time()\n",
        "download_time = end_time - start_time\n",
        "print(\"Temps de téléchargement : {} secondes\".format(download_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "#\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q findspark\n",
        "#"
      ],
      "metadata": {
        "id": "VGXbsmlZtr0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "#\n",
        "import findspark\n",
        "findspark.init()\n",
        "#### end head Google-colab"
      ],
      "metadata": {
        "id": "zKh1k9bmt0A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_fSRQJvt3mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La première étape cruciale dans l'analyse de données consiste en leur description. Souvent, les enquêtes se concentrent uniquement sur cette phase, qui offre une première compréhension des résultats ou permet d'identifier des relations entre les variables étudiées. Cette étape sert de base pour des analyses plus avancées, telles que la simplification des données (par exemple, via des analyses factorielles) ou leur regroupement en typologies. Elle est également essentielle pour des analyses plus sophistiquées visant à expliquer les données (comme les régressions, les analyses de variance ou les analyses conjointes).\n",
        "\n",
        "L'objectif de ce tutoriel est de présenter les principales méthodes de description des données afin d'effectuer une première analyse des données recueillies lors d'une enquête. Après avoir examiné la nature des variables, nous aborderons les tableaux croisés ainsi que les principaux tests statistiques qui y sont associés, y compris les tests d'hypothèses paramétriques et non paramétriques."
      ],
      "metadata": {
        "id": "GZ3Z34fcx30l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caractérisation d'une variable\n",
        "\n",
        "Le terme \"variable\" désigne l'ensemble des données observées sur différents individus pour une caractéristique donnée (Tenenhaus, 1996). Une variable est qualifiée de qualitative si elle est composée de modalités ; elle peut être de type nominale (lorsque les modalités ne présentent aucune structure particulière) ou ordinale (lorsque les modalités sont ordonnées). Une variable est considérée comme quantitative ou métrique si ses modalités peuvent être mesurées (par exemple l'âge, la valeur d'une action, etc.)."
      ],
      "metadata": {
        "id": "_l5Zr3gfyRjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYSE D'UNE VARIABLE QUALITATIVE\n",
        "\n",
        "L'analyse d'une variable qualitative implique la présentation des effectifs, représentant le nombre d'individus de l'échantillon pour chaque modalité de la variable, ainsi que des fréquences, indiquant le nombre de réponses associées à chaque modalité de la variable étudiée. Souvent, le responsable de l'étude cherche à répondre à une série de questions se rapportant à une seule et même variable."
      ],
      "metadata": {
        "id": "LvmietWnya_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYSE D'UNE VARIABLE QUANTITATIVE\n",
        "\n",
        "La description d'une variable quantitative implique l'examen de plusieurs critères :\n",
        "- **Mesures de la tendance centrale :** Ces mesures incluent la moyenne, la médiane et le mode.\n",
        "- **Mesures de la dispersion :** Elles comprennent l'étendue, la variance, l'écart type et le coefficient de variation.\n",
        "- **Mesures de la distribution :** Cela englobe l'asymétrie et l'aplatissement de la distribution.\n",
        "- **Représentations graphiques :** Des visualisations telles que les histogrammes ou les boîtes à moustaches sont souvent utilisées pour illustrer la distribution des données."
      ],
      "metadata": {
        "id": "m--5X6bzyh-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mesures de la tendance centrale\n",
        "\n",
        "Les mesures de la tendance centrale sont utilisées pour résumer une série d'observations en une valeur considérée comme représentative. La moyenne arithmétique est couramment employée, calculée en additionnant toutes les valeurs observées et en les divisant par le nombre total d'observations. Cependant, si des valeurs extrêmes affectent fortement la moyenne, il est préférable d'utiliser la médiane, qui est moins sensible aux valeurs aberrantes. La médiane représente la valeur au milieu de la distribution, séparant les données en deux parties égales. Enfin, le mode est la valeur qui apparaît le plus fréquemment dans la série de données. Si plusieurs valeurs ont la même fréquence maximale, elles sont toutes considérées comme des modes."
      ],
      "metadata": {
        "id": "AKGpXboJzMB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mesures de la dispersion\n",
        "\n",
        "Les mesures de dispersion utilisent plusieurs indicateurs pour quantifier la variabilité des données. L'étendue, également appelée intervalle, est la différence entre la plus grande et la plus petite valeur observée. La variance mesure la dispersion des données par rapport à la moyenne, calculée en faisant la somme des carrés des écarts par rapport à la moyenne, puis en divisant par le nombre d'observations moins un. Une faible variance indique que les données sont relativement proches de la moyenne, tandis qu'une variance élevée indique une dispersion importante des données autour de la moyenne. Cependant, la variance est sensible aux valeurs extrêmes. L'écart type, exprimé dans la même unité que la variable, est la racine carrée de la variance. Il offre une mesure de dispersion plus intuitive que la variance. Le coefficient de variation, exprimé en pourcentage, est le rapport de l'écart type à la moyenne. Il permet de comparer la variabilité relative entre des échantillons présentant des moyennes différentes, mais issus de la même distribution."
      ],
      "metadata": {
        "id": "pv1r-jMVzkqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics"
      ],
      "metadata": {
        "id": "4R0p6nhEt4Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la traduction des fonctions Spark MLlib que vous avez mentionnées :\n",
        "\n",
        "1. `chiSqTest(observed[, expected])`: Si observed est un vecteur, effectue un test du khi-deux de Pearson pour vérifier l'ajustement des données observées par rapport à la distribution attendue, ou par rapport à la distribution uniforme (par défaut), avec chaque catégorie ayant une fréquence attendue de 1 / len(observed).\n",
        "\n",
        "2. `colStats(rdd)`: Calcule les statistiques sommaires par colonne pour le RDD d'entrée [Vector].\n",
        "\n",
        "3. `corr(x[, y, method])`: Calcule la corrélation (matrice) pour les RDD(s) d'entrée en utilisant la méthode spécifiée.\n",
        "\n",
        "4. `kolmogorovSmirnovTest(data[, distName])`: Effectue le test de Kolmogorov-Smirnov (KS) pour des données échantillonnées à partir d'une distribution continue."
      ],
      "metadata": {
        "id": "MjayA2ABuEDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import ChiSquareTest\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Initialiser la session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ChiSquareTestExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "observed = Vectors.dense([4, 6, 5])\n",
        "pearson = Statistics.chiSqTest(observed)\n",
        "print(pearson.statistic)\n",
        "\n",
        "pearson.degreesOfFreedom\n",
        "\n",
        "print(round(pearson.pValue, 4))\n",
        "\n",
        "pearson.method\n",
        "\n",
        "pearson.nullHypothesis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Zs9Rux3dt6kQ",
        "outputId": "9b520907-b32d-456b-a317-e0f10efcbf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ChiSquareTest() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9612ea2d22b7>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mobserved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpearson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSquareTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ChiSquareTest() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.linalg import Vectors\n",
        "rdd = sc.parallelize([Vectors.dense([2, 0, 0, -2]),\n",
        "                      Vectors.dense([4, 5, 0,  3]),\n",
        "                      Vectors.dense([6, 7, 0,  8])])\n",
        "cStats = Statistics.colStats(rdd)\n",
        "cStats.mean()\n",
        "\n",
        "cStats.variance()\n",
        "\n",
        "cStats.count()\n",
        "\n",
        "cStats.numNonzeros()\n",
        "\n",
        "cStats.max()\n",
        "\n",
        "cStats.min()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "G9cpuf1gul3I",
        "outputId": "84130792-b643-4c88-9fd3-098ebbd47007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7d84c1bc5c00>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rdd = sc.parallelize([Vectors.dense([2, 0, 0, -2]),\n\u001b[0m\u001b[1;32m      3\u001b[0m                       \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       Vectors.dense([6, 7, 0,  8])])\n\u001b[1;32m      5\u001b[0m \u001b[0mcStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.stat import ANOVA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.types import StructType, StructField, FloatType\n",
        "\n",
        "# Initialiser la session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ANOVAExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Créer un schéma pour le DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"label\", FloatType(), True),\n",
        "    StructField(\"features\", FloatType(), True)\n",
        "])\n",
        "\n",
        "# Créer un DataFrame avec des données artificielles\n",
        "data = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Créer une instance de la classe ANOVA\n",
        "anova_test = ANOVA()\n",
        "\n",
        "# Effectuer l'ANOVA\n",
        "result = anova_test.fit(df).anova(df)\n",
        "\n",
        "# Afficher les résultats\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "CDJrtzRc0Yro",
        "outputId": "940a9072-8a2f-48c3-c051-e52c06fadf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'ANOVA' from 'pyspark.ml.stat' (/content/spark-3.5.0-bin-hadoop3/python/pyspark/ml/stat.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a3b578f2d614>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mANOVA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ANOVA' from 'pyspark.ml.stat' (/content/spark-3.5.0-bin-hadoop3/python/pyspark/ml/stat.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vérifier"
      ],
      "metadata": {
        "id": "sUJtIJVKJDi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Vérifier le Type de Données des Colonnes dans un DataFrame"
      ],
      "metadata": {
        "id": "8goyInxzBwI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Vérifier le Type de Données des Colonnes dans un DataFrame\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes en PySpark pour vérifier le type de données des colonnes dans un DataFrame :\n",
        "\n",
        "**Méthode 1 : Vérifier le Type de Données d'une Colonne Spécifique**\n",
        "\n",
        "```python\n",
        "# Retourner le type de données de la colonne 'conference'\n",
        "dict(df.dtypes)['conference']\n",
        "```\n",
        "\n",
        "**Méthode 2 : Vérifier le Type de Données de Toutes les Colonnes**\n",
        "\n",
        "```python\n",
        "# Retourner le type de données de toutes les colonnes\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Est', 11, 4],\n",
        "           ['A', None, 8, 9],\n",
        "           ['A', 'Est', 10, 3],\n",
        "           ['B', 'Ouest', None, 12],\n",
        "           ['B', 'Ouest', None, 4],\n",
        "           ['C', 'Est', 5, 2]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'conference', 'points', 'passes']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+------+----------+------+-------+\n",
        "|equipe|conference|points|passes|\n",
        "+------+----------+------+-------+\n",
        "|     A|       Est|    11|      4|\n",
        "|     A|      null|     8|      9|\n",
        "|     A|       Est|    10|      3|\n",
        "|     B|     Ouest|  null|     12|\n",
        "|     B|     Ouest|  null|      4|\n",
        "|     C|       Est|     5|      2|\n",
        "+------+----------+------+-------+\n",
        "```\n",
        "\n",
        "**Exemple 1 : Vérifier le Type de Données d'une Colonne Spécifique**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour vérifier le type de données de la colonne conference dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Retourner le type de données de la colonne 'conference'\n",
        "dict(df.dtypes)['conference']\n",
        "```\n",
        "\n",
        "```\n",
        "'string'\n",
        "```\n",
        "\n",
        "La sortie nous indique que la colonne conference a un type de données string.\n",
        "\n",
        "Pour vérifier le type de données d'une autre colonne spécifique, remplacez simplement conference par un autre nom de colonne :\n",
        "\n",
        "```python\n",
        "# Retourner le type de données de la colonne 'points'\n",
        "dict(df.dtypes)['points']\n",
        "```\n",
        "\n",
        "```\n",
        "'bigint'\n",
        "```\n",
        "\n",
        "La sortie nous indique que la colonne points a un type de données bigint.\n",
        "\n",
        "**Exemple 2 : Vérifier le Type de Données de Toutes les Colonnes**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour vérifier le type de données de toutes les colonnes dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Retourner le type de données de toutes les colonnes\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "```\n",
        "[('equipe', 'string'),\n",
        " ('conference', 'string'),\n",
        " ('points', 'bigint'),\n",
        " ('passes', 'bigint')]\n",
        "```\n",
        "\n",
        "La sortie montre chacun des noms de colonnes ainsi que le type de données de chaque colonne.\n",
        "\n",
        "Par exemple, nous pouvons voir que :\n",
        "\n",
        "- La colonne equipe a un type de données string.\n",
        "- La colonne conference a un type de données string.\n",
        "- La colonne points a un type de données bigint.\n",
        "- La colonne passes a un type de données bigint.\n",
        "- Et ainsi de suite."
      ],
      "metadata": {
        "id": "cvQUnWrC7bL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculer"
      ],
      "metadata": {
        "id": "JeP5P7pAH02K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Calculer la Moyenne d'une Colonne\n"
      ],
      "metadata": {
        "id": "M0ds5zEgBfPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Calculer la Moyenne d'une Colonne\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes pour calculer la moyenne d'une colonne dans un DataFrame PySpark :\n",
        "\n",
        "**Méthode 1 : Calculer la Moyenne pour une Colonne Spécifique**\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculer la moyenne de la colonne nommée 'game1'\n",
        "df.agg(F.mean('game1')).collect()[0][0]\n",
        "```\n",
        "\n",
        "**Méthode 2 : Calculer la Moyenne pour Plusieurs Colonnes**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Calculer la moyenne pour les colonnes game1, game2 et game3\n",
        "df.select(mean(df.game1), mean(df.game2), mean(df.game3)).show()\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "data = [['Mavs', 25, 11, 10],\n",
        "        ['Nets', 22, 8, 14],\n",
        "        ['Hawks', 14, 22, 10],\n",
        "        ['Kings', 30, 22, 35],\n",
        "        ['Bulls', 15, 14, 12],\n",
        "        ['Blazers', 10, 14, 18]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'game1', 'game2', 'game3']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(data, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+-------+-----+-----+-----+\n",
        "| equipe|game1|game2|game3|\n",
        "+-------+-----+-----+-----+\n",
        "|   Mavs|   25|   11|   10|\n",
        "|   Nets|   22|    8|   14|\n",
        "|  Hawks|   14|   22|   10|\n",
        "|  Kings|   30|   22|   35|\n",
        "|  Bulls|   15|   14|   12|\n",
        "|Blazers|   10|   14|   18|\n",
        "+-------+-----+-----+-----+\n",
        "```\n",
        "\n",
        "**Exemple 1 : Calculer la Moyenne pour une Colonne Spécifique**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer la moyenne des valeurs dans la colonne game1 du DataFrame uniquement :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculer la moyenne de la colonne nommée 'game1'\n",
        "df.agg(F.mean('game1')).collect()[0][0]\n",
        "```\n",
        "\n",
        "La moyenne des valeurs dans la colonne game1 s'avère être 19.333.\n",
        "\n",
        "Nous pouvons vérifier que ceci est correct en calculant manuellement la moyenne des valeurs dans cette colonne :\n",
        "\n",
        "Moyenne des valeurs dans game1 : (25 + 22 + 14 + 30 + 15 + 10) / 6 = 19.333.\n",
        "\n",
        "**Exemple 2 : Calculer la Moyenne pour Plusieurs Colonnes**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer la moyenne des valeurs pour les colonnes game1, game2 et game3 du DataFrame :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Calculer la moyenne pour les colonnes game1, game2 et game3\n",
        "df.select(mean(df.game1), mean(df.game2), mean(df.game3)).show()\n",
        "```\n",
        "\n",
        "```\n",
        "+------------------+------------------+----------+\n",
        "|        avg(game1)|        avg(game2)|avg(game3)|\n",
        "+------------------+------------------+----------+\n",
        "|19.333333333333332|15.166666666666666|      16.5|\n",
        "+------------------+------------------+----------+\n",
        "```\n",
        "\n",
        "Remarquez que trois nouvelles colonnes ont été ajoutées au DataFrame dont les valeurs sont basées sur les valeurs existantes dans la colonne points."
      ],
      "metadata": {
        "id": "KSI9KCja8LP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Calculer la Somme de Plusieurs Colonnes"
      ],
      "metadata": {
        "id": "OgOy7Y3k5LMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Calculer la Somme de Plusieurs Colonnes\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour calculer la somme des valeurs à travers plusieurs colonnes dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Définir les colonnes à additionner\n",
        "colonnes_a_additionner = ['game1', 'game2', 'game3']\n",
        "\n",
        "# Créer un nouveau DataFrame contenant la somme des colonnes spécifiques\n",
        "df_nouveau = df.withColumn('somme', F.expr('+'.join(colonnes_a_additionner)))\n",
        "```\n",
        "\n",
        "Cet exemple crée une nouvelle colonne appelée somme qui contient la somme des valeurs à travers les colonnes game1, game2 et game3 dans le DataFrame.\n",
        "\n",
        "**Exemple**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les points marqués par différents joueurs de basket lors de trois matchs différents :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['Mavs', 25, 11, 10],\n",
        "           ['Nets', 22, 8, 14],\n",
        "           ['Hawks', 14, 22, 10],\n",
        "           ['Kings', 30, 22, 35],\n",
        "           ['Bulls', 15, 14, 12],\n",
        "           ['Blazers', 10, 14, 18]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'match1', 'match2', 'match3']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+-------+------+------+------+\n",
        "| equipe|match1|match2|match3|\n",
        "+-------+------+------+------+\n",
        "|   Mavs|    25|    11|    10|\n",
        "|   Nets|    22|     8|    14|\n",
        "|  Hawks|    14|    22|    10|\n",
        "|  Kings|    30|    22|    35|\n",
        "|  Bulls|    15|    14|    12|\n",
        "|Blazers|    10|    14|    18|\n",
        "+-------+------+------+------+\n",
        "```\n",
        "\n",
        "Supposons que nous voulions ajouter une nouvelle colonne appelée somme qui contient la somme des points marqués par chaque joueur à travers les trois matchs.\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour le faire :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Définir les colonnes à additionner\n",
        "colonnes_a_additionner = ['match1', 'match2', 'match3']\n",
        "\n",
        "# Créer un nouveau DataFrame contenant la somme des colonnes spécifiques\n",
        "df_nouveau = df.withColumn('somme', F.expr('+'.join(colonnes_a_additionner)))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_nouveau.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+-------+------+------+------+-----+\n",
        "| equipe|match1|match2|match3|somme|\n",
        "+-------+------+------+------+-----+\n",
        "|   Mavs|    25|    11|    10|   46|\n",
        "|   Nets|    22|     8|    14|   44|\n",
        "|  Hawks|    14|    22|    10|   46|\n",
        "|  Kings|    30|    22|    35|   87|\n",
        "|  Bulls|    15|    14|    12|   41|\n",
        "|Blazers|    10|    14|    18|   42|\n",
        "+-------+------+------+------+-----+\n",
        "```\n",
        "\n",
        "Remarquez que la nouvelle colonne somme contient la somme des valeurs à travers les colonnes match1, match2 et match3.\n",
        "\n",
        "Par exemple :\n",
        "\n",
        "- La somme des points pour le joueur Mavs est 25 + 11 + 10 = 46\n",
        "- La somme des points pour le joueur Nets est 22 + 8 + 14 = 44\n",
        "- La somme des points pour le joueur Hawks est 14 + 22 + 10 = 46\n",
        "- Et ainsi de suite.\n",
        "\n",
        "Notez que nous avons utilisé la fonction withColumn pour retourner un nouveau DataFrame avec la colonne somme ajoutée et toutes les autres colonnes inchangées."
      ],
      "metadata": {
        "id": "wpTj6Huf4KL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Calculer le Pourcentage du Total avec groupBy"
      ],
      "metadata": {
        "id": "HOmqPfUJBZ2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark : Calculer le Pourcentage du Total avec groupBy\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour calculer le pourcentage du nombre total de lignes que chaque groupe représente dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "# Calculer le nombre total de lignes dans le DataFrame\n",
        "n = df.count()\n",
        "\n",
        "# Calculer le pourcentage du nombre total de lignes pour chaque équipe\n",
        "df.groupBy('team').count().withColumn('team_percent', (F.col('count')/n)*100).show()\n",
        "```\n",
        "\n",
        "Cet exemple compte le nombre d'occurrences pour chaque valeur unique dans la colonne de l'équipe, puis calcule le pourcentage du nombre total de lignes que chaque valeur unique représente.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple**\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les points marqués par divers joueurs de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Guard', 11],\n",
        "           ['A', 'Guard', 8],\n",
        "           ['A', 'Forward', 22],\n",
        "           ['A', 'Forward', 22],\n",
        "           ['B', 'Guard', 14],\n",
        "           ['B', 'Guard', 14],\n",
        "           ['B', 'Forward', 13],\n",
        "           ['C', 'Forward', 7]]\n",
        "  \n",
        "# Définir les noms des colonnes\n",
        "colonnes = ['equipe', 'position', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour compter le nombre d'occurrences de chaque valeur unique dans la colonne de l'équipe, puis calculer le pourcentage du nombre total de lignes que chaque valeur d'équipe unique représente :\n",
        "\n",
        "```python\n",
        "# Calculer le nombre total de lignes dans le DataFrame\n",
        "n = df.count()\n",
        "\n",
        "# Calculer le pourcentage du nombre total de lignes pour chaque équipe\n",
        "df.groupBy('equipe').count().withColumn('pourcentage_equipe', (F.col('count')/n)*100).show()\n",
        "```\n",
        "\n",
        "La colonne `pourcentage_equipe` montre le pourcentage du nombre total de lignes représenté par chaque équipe unique.\n",
        "\n",
        "Par exemple, il y a 8 lignes au total dans le DataFrame.\n",
        "\n",
        "À partir de la colonne `pourcentage_equipe`, nous pouvons voir :\n",
        "\n",
        "- Il y a 4 occurrences de l'équipe A, ce qui représente 4/8 = 50 % du nombre total de lignes.\n",
        "- Il y a 3 occurrences de l'équipe B, ce qui représente 3/8 = 37,5 % du nombre total de lignes.\n",
        "- Il y a 1 occurrence de l'équipe C, ce qui représente 1/8 = 12,5 % du nombre total de lignes.\n"
      ],
      "metadata": {
        "id": "YqcmkJHE9yuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment calculer l'écart-type en PySpark"
      ],
      "metadata": {
        "id": "ET8pgSsjVEKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment calculer l'écart-type en PySpark\n",
        "*PAR ZACH BOBBITT PUBLIÉ LE 26 OCTOBRE 2023*\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes pour calculer l'écart-type d'une colonne dans un DataFrame PySpark :\n",
        "\n",
        "**Méthode 1 : Calculer l'écart-type pour une colonne spécifique**\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculer l'écart-type des valeurs dans la colonne 'game1'\n",
        "df.agg(F.stddev('game1')).collect()[0][0]\n",
        "```\n",
        "\n",
        "**Méthode 2 : Calculer l'écart-type pour plusieurs colonnes**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import stddev\n",
        "\n",
        "# Calculer l'écart-type pour les colonnes game1, game2 et game3\n",
        "df.select(stddev(df.game1), stddev(df.game2), stddev(df.game3)).show()\n",
        "```\n",
        "\n",
        "Remarque : La fonction stddev utilise la formule de l'écart-type de l'échantillon pour calculer l'écart-type. Si vous préférez utiliser la formule de l'écart-type de la population, utilisez plutôt la fonction stddev_pop.\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['Mavs', 25, 11, 10],\n",
        "           ['Nets', 22, 8, 14],\n",
        "           ['Hawks', 14, 22, 10],\n",
        "           ['Kings', 30, 22, 35],\n",
        "           ['Bulls', 15, 14, 12],\n",
        "           ['Blazers', 10, 14, 18]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['team', 'game1', 'game2', 'game3']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Calculer l'écart-type pour une colonne spécifique**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer l'écart-type des valeurs dans la colonne game1 du DataFrame uniquement :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculer l'écart-type de la colonne nommée 'game1'\n",
        "df.agg(F.stddev('game1')).collect()[0][0]\n",
        "```\n",
        "\n",
        "L'écart-type des valeurs dans la colonne game1 s'avère être 7.5807.\n",
        "\n",
        "**Exemple 2 : Calculer l'écart-type pour plusieurs colonnes**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer l'écart-type des valeurs des colonnes game1, game2 et game3 du DataFrame :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import stddev\n",
        "\n",
        "# Calculer l'écart-type pour les colonnes game1, game2 et game3\n",
        "df.select(stddev(df.game1), stddev(df.game2), stddev(df.game3)).show()\n",
        "```\n",
        "\n",
        "Dans la sortie, nous pouvons voir :\n",
        "\n",
        "- L'écart-type des valeurs dans la colonne game1 est 7.5807.\n",
        "- L'écart-type des valeurs dans la colonne game2 est 5.7417.\n",
        "- L'écart-type des valeurs dans la colonne game3 est 9.5446."
      ],
      "metadata": {
        "id": "msouO6_4VEVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation de groupBy avec Count Distinct en PySpark"
      ],
      "metadata": {
        "id": "EUVNnLmuVEgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilisation de groupBy avec Count Distinct en PySpark\n",
        "*PAR ZACH BOBBITT PUBLIÉ LE 30 OCTOBRE 2023*\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour compter le nombre de valeurs distinctes dans une colonne d'un DataFrame PySpark, regroupées par une autre colonne :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "df.groupBy('team').agg(countDistinct('points')).show()\n",
        "```\n",
        "\n",
        "Cet exemple particulier calcule le nombre de valeurs distinctes dans la colonne points, regroupées par les valeurs de la colonne team.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Utilisation de groupBy avec Count Distinct en PySpark**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant contenant des informations sur les points marqués par différents joueurs de basket :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Guard', 11],\n",
        "           ['A', 'Guard', 8],\n",
        "           ['A', 'Forward', 22],\n",
        "           ['A', 'Forward', 22],\n",
        "           ['B', 'Guard', 14],\n",
        "           ['B', 'Guard', 14],\n",
        "           ['B', 'Forward', 13],\n",
        "           ['B', 'Forward', 14],\n",
        "           ['C', 'Forward', 23],\n",
        "           ['C', 'Guard', 30]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['team', 'position', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer le nombre de valeurs distinctes dans la colonne points, regroupées par les valeurs de la colonne team :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# Calculer les valeurs distinctes dans la colonne points, regroupées par la colonne team\n",
        "df.groupBy('team').agg(countDistinct('points')).show()\n",
        "```\n",
        "\n",
        "Le DataFrame résultant montre le nombre de valeurs distinctes dans la colonne points, regroupées par les valeurs de la colonne team.\n",
        "\n",
        "Par exemple, nous pouvons voir :\n",
        "\n",
        "- Il y a 2 valeurs distinctes dans la colonne points pour l'équipe B.\n",
        "- Il y a 2 valeurs distinctes dans la colonne points pour l'équipe C.\n",
        "- Il y a 3 valeurs distinctes dans la colonne points pour l'équipe A.\n",
        "\n",
        "Si vous souhaitez donner un autre nom à la colonne count(points), vous pouvez utiliser la fonction alias comme suit :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# Calculer les valeurs distinctes dans la colonne points, regroupées par la colonne team\n",
        "df.groupBy('team').agg(countDistinct('points').alias('distinct_points')).show()\n",
        "```\n",
        "\n",
        "Le DataFrame résultant montre le nombre de valeurs distinctes de points pour chaque équipe avec la colonne distincte maintenant nommée distinct_points, comme nous l'avons spécifié dans la fonction alias."
      ],
      "metadata": {
        "id": "gsPN0VNeVEry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculer la différence entre les lignes en PySpark"
      ],
      "metadata": {
        "id": "RxUQhxPnVEvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ark DataFrame API [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html).\n",
        "\n",
        "### Calculer la différence entre les lignes en PySpark\n",
        "*PAR ZACH BOBBITT PUBLIÉ LE 3 NOVEMBRE 2023*\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour calculer la différence entre les lignes dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Définir la fenêtre\n",
        "w = Window.partitionBy('employee').orderBy('period')\n",
        "\n",
        "# Calculer la différence entre les lignes des valeurs de ventes, groupées par employé\n",
        "df_new = df.withColumn('sales_diff', F.col('sales') - F.lag(F.col('sales'), 1).over(w))\n",
        "```\n",
        "\n",
        "Cet exemple particulier calcule la différence de valeurs entre les lignes consécutives dans la colonne des ventes, groupées par les valeurs de la colonne employé.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Calcul de la différence entre les lignes en PySpark**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant contenant des informations sur les ventes réalisées par divers employés dans une entreprise lors de différentes périodes de vente :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 1, 18],\n",
        "           ['A', 2, 20],\n",
        "           ['A', 3, 25],\n",
        "           ['A', 4, 40],\n",
        "           ['B', 1, 34],\n",
        "           ['B', 2, 32],\n",
        "           ['B', 3, 19]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['employee', 'period', 'sales']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer la différence de valeurs entre les lignes consécutives dans la colonne des ventes, groupées par les valeurs de la colonne employé :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Définir la fenêtre\n",
        "w = Window.partitionBy('employee').orderBy('period')\n",
        "\n",
        "# Calculer la différence entre les lignes des valeurs de ventes, groupées par employé\n",
        "df_new = df.withColumn('sales_diff', F.col('sales') - F.lag(F.col('sales'), 1).over(w))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_new.show()\n",
        "```\n",
        "\n",
        "La nouvelle colonne appelée sales_diff montre la différence de valeurs entre les lignes consécutives dans la colonne des ventes.\n",
        "\n",
        "Notez que la première valeur dans la colonne sales_diff pour chaque employé est nulle car il n'y a pas de valeur précédente pour calculer la différence.\n",
        "\n",
        "Si vous le souhaitez, vous pouvez utiliser la fonction fillna pour remplacer ces valeurs nulles par zéro :\n",
        "\n",
        "```python\n",
        "# Remplacer les valeurs nulles par 0 dans la colonne sales_diff\n",
        "df_new.fillna(0, 'sales_diff').show()\n",
        "```\n",
        "\n",
        "Chacune des valeurs nulles dans la colonne sales_diff a maintenant été remplacée par zéro."
      ],
      "metadata": {
        "id": "ov-MCMcnVE1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment calculer le mode d'une colonne"
      ],
      "metadata": {
        "id": "8tWIMQwGVE99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment calculer le mode d'une colonne\n",
        "Par Zach Bobbitt, publié le 3 novembre 2023\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes pour calculer le mode d'une colonne dans un DataFrame PySpark :\n",
        "\n",
        "**Méthode 1 : Calculer le mode pour une colonne spécifique**\n",
        "\n",
        "```python\n",
        "# Calculer le mode de la colonne 'conference'\n",
        "df.groupby('conference').count().orderBy('count', ascending=False).first()[0]\n",
        "```\n",
        "\n",
        "**Méthode 2 : Calculer le mode pour toutes les colonnes**\n",
        "\n",
        "```python\n",
        "# Calculer le mode de chaque colonne dans le DataFrame\n",
        "[[i, df.groupby(i).count().orderBy('count', ascending=False).first()[0]] for i in df.columns]\n",
        "```\n",
        "\n",
        "Voici comment utiliser ces méthodes avec un exemple de DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "data = [['A', 'East', 11, 4],\n",
        "        ['A', 'East', 8, 9],\n",
        "        ['A', 'East', 10, 3],\n",
        "        ['B', 'West', 6, 12],\n",
        "        ['B', 'West', 6, 4],\n",
        "        ['C', 'East', 5, 2]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "  \n",
        "# Créer le DataFrame avec les données et les noms de colonnes\n",
        "df = spark.createDataFrame(data, columns)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Calculer le mode pour une colonne spécifique**\n",
        "\n",
        "```python\n",
        "# Calculer le mode de la colonne 'conference'\n",
        "mode_conference = df.groupby('conference').count().orderBy('count', ascending=False).first()[0]\n",
        "\n",
        "print(mode_conference)\n",
        "```\n",
        "\n",
        "Résultat :\n",
        "```\n",
        "'East'\n",
        "```\n",
        "\n",
        "Le mode de la colonne 'conference' est 'East', ce qui indique la valeur la plus fréquente.\n",
        "\n",
        "**Exemple 2 : Calculer le mode pour toutes les colonnes**\n",
        "\n",
        "```python\n",
        "# Calculer le mode de chaque colonne dans le DataFrame\n",
        "mode_columns = [[i, df.groupby(i).count().orderBy('count', ascending=False).first()[0]] for i in df.columns]\n",
        "\n",
        "print(mode_columns)\n",
        "```\n",
        "\n",
        "Résultat :\n",
        "```\n",
        "[['team', 'A'], ['conference', 'East'], ['points', 6], ['assists', 4]]\n",
        "```\n",
        "\n",
        "La sortie montre le mode pour chaque colonne dans le DataFrame. Par exemple :\n",
        "\n",
        "- Le mode de la colonne 'team' est 'A'\n",
        "- Le mode de la colonne 'conference' est 'East'\n",
        "- Le mode de la colonne 'points' est 6\n",
        "- Le mode de la colonne 'assists' est 4\n",
        "\n",
        "Note : Dans les deux exemples, nous avons utilisé les fonctions groupby et count pour compter les occurrences de chaque valeur unique dans la colonne, puis nous avons simplement extrait la valeur avec le comptage le plus fréquent pour obtenir le mode."
      ],
      "metadata": {
        "id": "BDIq-_0AVFBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comment calculer la valeur minimale à travers les colonnes dans PySpark**"
      ],
      "metadata": {
        "id": "Ib6d-wbeVFEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comment calculer la valeur minimale à travers les colonnes dans PySpark**\n",
        "Par Zach Bobbitt, publié le 17 octobre 2023\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour calculer la valeur minimale à travers plusieurs colonnes dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import least\n",
        "\n",
        "# Trouver la valeur minimale à travers les colonnes 'game1', 'game2' et 'game3'\n",
        "df_new = df.withColumn('min', least('game1', 'game2', 'game3'))\n",
        "```\n",
        "\n",
        "Cet exemple particulier crée une nouvelle colonne appelée min qui contient le minimum des valeurs à travers les colonnes game1, game2 et game3 dans le DataFrame.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment calculer la valeur minimale à travers les colonnes dans PySpark**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant contenant des informations sur les points marqués par divers joueurs de basketball lors de trois jeux différents :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "data = [['Mavs', 25, 11, 10],\n",
        "        ['Nets', 22, 8, 14],\n",
        "        ['Hawks', 14, 22, 10],\n",
        "        ['Kings', 30, 22, 35],\n",
        "        ['Bulls', 15, 14, 12],\n",
        "        ['Blazers', 10, 14, 18]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "columns = ['team', 'game1', 'game2', 'game3']\n",
        "  \n",
        "# Créer le DataFrame avec les données et les noms de colonnes\n",
        "df = spark.createDataFrame(data, columns)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Supposons que nous voulions ajouter une nouvelle colonne appelée min qui contient le minimum des points marqués par chaque joueur à travers les trois jeux.\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour le faire :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import least\n",
        "\n",
        "# Trouver la valeur minimale à travers les colonnes 'game1', 'game2' et 'game3'\n",
        "df_new = df.withColumn('min', least('game1', 'game2', 'game3'))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_new.show()\n",
        "```\n",
        "\n",
        "Remarquez que la nouvelle colonne min contient le minimum des valeurs à travers les colonnes game1, game2 et game3.\n",
        "\n",
        "Par exemple :\n",
        "\n",
        "- Le minimum de points pour le joueur Mavs est 10\n",
        "- Le minimum de points pour le joueur Nets est 8\n",
        "- Le minimum de points pour le joueur Hawks est 10\n",
        "Et ainsi de suite.\n",
        "\n",
        "Notez que nous avons utilisé la fonction withColumn pour retourner un nouveau DataFrame avec la colonne min ajoutée et toutes les autres colonnes laissées inchangées."
      ],
      "metadata": {
        "id": "831SeeQwVFGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comment Calculer la Valeur Minimum d'une Colonne en PySpark**"
      ],
      "metadata": {
        "id": "Ub5S46f6VFJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comment Calculer la Valeur Minimum d'une Colonne en PySpark**\n",
        "Par Zach Bobbitt, publié le 17 octobre 2023\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes pour calculer la valeur minimale d'une colonne dans un DataFrame PySpark :\n",
        "\n",
        "**Méthode 1 : Calculer le Minimum pour une Colonne Spécifique**\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculer le minimum de la colonne nommée 'game1'\n",
        "df.agg(F.min('game1')).collect()[0][0]\n",
        "```\n",
        "\n",
        "**Méthode 2 : Calculer le Minimum pour Plusieurs Colonnes**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import min\n",
        "\n",
        "# Calculer le minimum pour les colonnes game1, game2 et game3\n",
        "df.select(min(df.game1), min(df.game2), min(df.game3)).show()\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "data = [['Mavs', 25, 11, 10],\n",
        "        ['Nets', 22, 8, 14],\n",
        "        ['Hawks', 14, 22, 10],\n",
        "        ['Kings', 30, 22, 35],\n",
        "        ['Bulls', 15, 14, 12],\n",
        "        ['Blazers', 10, 14, 18]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "columns = ['team', 'game1', 'game2', 'game3']\n",
        "  \n",
        "# Créer le DataFrame avec les données et les noms de colonnes\n",
        "df = spark.createDataFrame(data, columns)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Calculer le Minimum pour une Colonne Spécifique**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer le minimum des valeurs dans la colonne game1 du DataFrame uniquement :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculer le minimum de la colonne nommée 'game1'\n",
        "df.agg(F.min('game1')).collect()[0][0]\n",
        "```\n",
        "\n",
        "Le minimum des valeurs dans la colonne game1 est donc 10.\n",
        "\n",
        "Nous pouvons vérifier que cela est correct en identifiant manuellement le minimum des valeurs dans cette colonne :\n",
        "\n",
        "Toutes les valeurs dans la colonne game1 : 10, 14, 15, 22, 25, 30\n",
        "\n",
        "Nous pouvons voir que 10 est effectivement la valeur minimale dans la colonne.\n",
        "\n",
        "**Exemple 2 : Calculer le Minimum pour Plusieurs Colonnes**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer le minimum des valeurs pour les colonnes game1, game2 et game3 du DataFrame :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import min\n",
        "\n",
        "# Calculer le minimum pour les colonnes game1, game2 et game3\n",
        "df.select(min(df.game1), min(df.game2), min(df.game3)).show()\n",
        "```\n",
        "\n",
        "À partir de la sortie, nous pouvons voir que :\n",
        "\n",
        "- Le minimum des valeurs dans la colonne game1 est 10.\n",
        "- Le minimum des valeurs dans la colonne game2 est 8.\n",
        "- Le minimum des valeurs dans la colonne game3 est 10."
      ],
      "metadata": {
        "id": "0X3blSbJVFMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comment Calculer la Valeur Maximale d'une Colonne en PySpark**"
      ],
      "metadata": {
        "id": "YbK9uk6oVFZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comment Calculer la Valeur Maximale d'une Colonne en PySpark**\n",
        "Par Zach Bobbitt, publié le 17 octobre 2023\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour calculer la valeur maximale à travers plusieurs colonnes dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import greatest\n",
        "\n",
        "# Trouver la valeur maximale à travers les colonnes 'game1', 'game2' et 'game3'\n",
        "df_new = df.withColumn('max', greatest('game1', 'game2', 'game3'))\n",
        "```\n",
        "\n",
        "Cet exemple particulier crée une nouvelle colonne appelée max qui contient le maximum des valeurs à travers les colonnes game1, game2 et game3 dans le DataFrame.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment Calculer la Valeur Maximale à Travers les Colonnes en PySpark**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les points marqués par différents joueurs de basketball lors de trois jeux différents :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "data = [['Mavs', 25, 11, 10],\n",
        "        ['Nets', 22, 8, 14],\n",
        "        ['Hawks', 14, 22, 10],\n",
        "        ['Kings', 30, 22, 35],\n",
        "        ['Bulls', 15, 14, 12],\n",
        "        ['Blazers', 10, 14, 18]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "columns = ['team', 'game1', 'game2', 'game3']\n",
        "  \n",
        "# Créer le DataFrame avec les données et les noms de colonnes\n",
        "df = spark.createDataFrame(data, columns)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Supposons que nous voulions ajouter une nouvelle colonne appelée max qui contient le maximum des points marqués par chaque joueur à travers les trois jeux.\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour le faire :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import greatest\n",
        "\n",
        "# Trouver la valeur maximale à travers les colonnes 'game1', 'game2' et 'game3'\n",
        "df_new = df.withColumn('max', greatest('game1', 'game2', 'game3'))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_new.show()\n",
        "```\n",
        "\n",
        "Remarquez que la nouvelle colonne max contient le maximum des valeurs à travers les colonnes game1, game2 et game3.\n",
        "\n",
        "Par exemple :\n",
        "\n",
        "- Le maximum de points pour le joueur Mavs est 25\n",
        "- Le maximum de points pour le joueur Nets est 22\n",
        "- Le maximum de points pour le joueur Hawks est 22\n",
        "Et ainsi de suite.\n",
        "\n",
        "Notez que nous avons utilisé la fonction withColumn pour renvoyer un nouveau DataFrame avec la colonne max ajoutée et toutes les autres colonnes laissées inchangées."
      ],
      "metadata": {
        "id": "5iJQGVrwXXtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comment Calculer la Somme de Chaque Ligne dans un DataFrame PySpark**"
      ],
      "metadata": {
        "id": "G4lCTyZhXYDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comment Calculer la Somme de Chaque Ligne dans un DataFrame PySpark**\n",
        "Par Zach Bobbitt, publié le 9 novembre 2023\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour calculer la somme des valeurs dans chaque ligne d'un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Ajouter une nouvelle colonne contenant la somme de chaque ligne\n",
        "df_new = df.withColumn('row_sum', sum([F.col(c) for c in df.columns]))\n",
        "```\n",
        "\n",
        "Cet exemple particulier crée une nouvelle colonne nommée row_sum qui contient la somme des valeurs dans chaque ligne.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment Calculer la Somme de Chaque Ligne en PySpark**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui montre le nombre de points marqués dans trois jeux différents par différents joueurs de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "data = [[14, 16, 10],\n",
        "        [12, 10, 13],\n",
        "        [8, 10, 20],\n",
        "        [15, 15, 15],\n",
        "        [19, 3, 15],\n",
        "        [24, 40, 23],\n",
        "        [15, 12, 19],\n",
        "        [10, 10, 16]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "columns = ['game1', 'game2', 'game3']\n",
        "  \n",
        "# Créer le DataFrame avec les données et les noms de colonnes\n",
        "df = spark.createDataFrame(data, columns)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour créer une nouvelle colonne nommée row_sum qui contient la somme des valeurs dans chaque ligne :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Ajouter une nouvelle colonne contenant la somme de chaque ligne\n",
        "df_new = df.withColumn('row_sum', sum([F.col(c) for c in df.columns]))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_new.show()\n",
        "```\n",
        "\n",
        "La nouvelle colonne row_sum contient la somme des valeurs dans chaque ligne.\n",
        "\n",
        "Par exemple :\n",
        "\n",
        "- La somme des valeurs dans la première ligne est 14 + 16 + 10 = 40.\n",
        "- La somme des valeurs dans la deuxième ligne est 12 + 10 + 13 = 35.\n",
        "- La somme des valeurs dans la troisième ligne est 8 + 10 + 20 = 38.\n",
        "Et ainsi de suite.\n",
        "\n",
        "Note : Si des valeurs nulles sont présentes dans la colonne, la fonction de somme ignorera ces valeurs par défaut."
      ],
      "metadata": {
        "id": "IQxMyjOdXYGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convertion"
      ],
      "metadata": {
        "id": "Lu5JBIyQIEr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Convertir un Timestamp en Date"
      ],
      "metadata": {
        "id": "N8iZQ2ZwCLjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Convertir un Timestamp en Date\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour convertir une colonne de timestamp en une colonne de date dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "# Convertir la colonne de timestamp en colonne de date\n",
        "df = df.withColumn('ma_date', df['mon_timestamp'].cast(DateType()))\n",
        "```\n",
        "\n",
        "Cet exemple crée une nouvelle colonne appelée ma_date qui contient les valeurs de date à partir des valeurs de timestamp dans la colonne mon_timestamp.\n",
        "\n",
        "**Exemple**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les ventes effectuées à différents moments chez une entreprise :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['2023-01-15 04:14:22', 225],\n",
        "           ['2023-02-24 10:55:01', 260],\n",
        "           ['2023-07-14 18:34:59', 413],\n",
        "           ['2023-10-30 22:20:05', 368]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['ts', 'ventes']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "\n",
        "# Convertir la colonne de chaîne en timestamp\n",
        "df = df.withColumn('ts', F.to_timestamp('ts', 'yyyy-MM-dd HH:mm:ss'))\n",
        "\n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+-------------------+-----+\n",
        "|                 ts|sales|\n",
        "+-------------------+-----+\n",
        "|2023-01-15 04:14:22|  225|\n",
        "|2023-02-24 10:55:01|  260|\n",
        "|2023-07-14 18:34:59|  413|\n",
        "|2023-10-30 22:20:05|  368|\n",
        "+-------------------+-----+\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour afficher le type de données de chaque colonne dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Vérifier le type de données de chaque colonne\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "```\n",
        "[('ts', 'timestamp'), ('sales', 'bigint')]\n",
        "```\n",
        "\n",
        "Nous pouvons voir que la colonne ts a actuellement un type de données timestamp.\n",
        "\n",
        "Pour convertir cette colonne d'un timestamp en une date, nous pouvons utiliser la syntaxe suivante :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "# Créer une colonne de date à partir de la colonne de timestamp\n",
        "df = df.withColumn('nouvelle_date', df['ts'].cast(DateType()))\n",
        "\n",
        "# Afficher le DataFrame mis à jour\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+-------------------+-----+------------+\n",
        "|                 ts|sales| new_date   |\n",
        "+-------------------+-----+------------+\n",
        "|2023-01-15 04:14:22|  225|2023-01-15  |\n",
        "|2023-02-24 10:55:01|  260|2023-02-24  |\n",
        "|2023-07-14 18:34:59|  413|2023-07-14  |\n",
        "|2023-10-30 22:20:05|  368|2023-10-30  |\n",
        "+-------------------+-----+------------+\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser à nouveau la fonction dtypes pour afficher les types de données de chaque colonne dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Vérifier le type de données de chaque colonne\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "```\n",
        "[('ts', 'timestamp'), ('sales', 'bigint'), ('new_date', 'date')]\n",
        "```\n",
        "\n",
        "Nous pouvons voir que la colonne new_date a un type de données date.\n",
        "\n",
        "Nous avons réussi à créer une colonne de date à partir d'une colonne de timestamp."
      ],
      "metadata": {
        "id": "v0L65K_H5L_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Convertir une Chaîne de Caractères en Date"
      ],
      "metadata": {
        "id": "lGV0aArGB_mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Convertir une Chaîne de Caractères en Date\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour convertir une colonne de chaîne de caractères en une colonne de date dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Convertir la colonne de chaîne de caractères en colonne de date\n",
        "df = df.withColumn('ma_colonne_date', F.to_date('ma_colonne_date'))\n",
        "```\n",
        "\n",
        "Cet exemple particulier convertit les valeurs dans la colonne ma_colonne_date de chaînes de caractères en dates.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les ventes effectuées à différentes dates dans une entreprise :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['2023-01-15', 225],\n",
        "           ['2023-02-24', 260],\n",
        "           ['2023-07-14', 413],\n",
        "           ['2023-10-30', 368]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['date', 'ventes']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+----------+-----+\n",
        "|      date|sales|\n",
        "+----------+-----+\n",
        "|2023-01-15|  225|\n",
        "|2023-02-24|  260|\n",
        "|2023-07-14|  413|\n",
        "|2023-10-30|  368|\n",
        "+----------+-----+\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour afficher le type de données de chaque colonne dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Vérifier le type de données de chaque colonne\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "```\n",
        "[('date', 'string'), ('sales', 'bigint')]\n",
        "```\n",
        "\n",
        "Nous pouvons voir que la colonne date a actuellement un type de données de chaîne de caractères.\n",
        "\n",
        "Pour convertir cette colonne d'une chaîne de caractères en une date, nous pouvons utiliser la syntaxe suivante :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Convertir la colonne 'date' de chaîne de caractères en date\n",
        "df = df.withColumn('date', F.to_date('date'))\n",
        "\n",
        "# Afficher le DataFrame mis à jour\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+----------+-----+\n",
        "|      date|sales|\n",
        "+----------+-----+\n",
        "|2023-01-15|  225|\n",
        "|2023-02-24|  260|\n",
        "|2023-07-14|  413|\n",
        "|2023-10-30|  368|\n",
        "+----------+-----+\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser à nouveau la fonction dtypes pour afficher les types de données de chaque colonne dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Vérifier le type de données de chaque colonne\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "```\n",
        "[('date', 'date'), ('sales', 'bigint')]\n",
        "```\n",
        "\n",
        "Nous pouvons voir que la colonne date a maintenant un type de données date.\n",
        "\n",
        "Nous avons réussi à convertir une colonne de chaîne de caractères en une colonne de date."
      ],
      "metadata": {
        "id": "DJMQMMsc6X_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Convertir une Colonne de Booléens en Entier"
      ],
      "metadata": {
        "id": "YbCNVpTyBLEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Convertir une Colonne de Booléens en Entier\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour convertir une colonne de booléens en une colonne d'entiers en PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Convertir une colonne de booléens en colonne d'entiers\n",
        "df_nouveau = df.withColumn('colonne_entier', when(df.colonne_booleenne==True, 1).otherwise(0))\n",
        "```\n",
        "\n",
        "Cet exemple particulier convertit la colonne de booléens nommée `colonne_booleenne` en une colonne d'entiers nommée `colonne_entier`.\n",
        "\n",
        "Chacune des valeurs égales à True dans la colonne de booléens sera affichée comme 1 dans la colonne d'entiers. De même, chaque valeur égale à False dans la colonne de booléens sera affichée comme 0 dans la colonne d'entiers.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple**\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur différentes équipes de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['Mavs', 18, True],\n",
        "           ['Nets', 33, True],\n",
        "           ['Lakers', 12, False],\n",
        "           ['Kings', 15, True],\n",
        "           ['Hawks', 19, False],\n",
        "           ['Wizards', 24, False],\n",
        "           ['Magic', 28, True],\n",
        "           ['Jazz', 40, False],\n",
        "           ['Thunder', 24, False],\n",
        "           ['Spurs', 13, True]]\n",
        "  \n",
        "# Définir les noms des colonnes\n",
        "colonnes = ['equipe', 'points', 'playoffs']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour créer une nouvelle colonne appelée `playoffs_int` qui convertit chacune des valeurs booléennes de true et false en les valeurs entières de 1 ou 0 :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Convertir une colonne de booléens en colonne d'entiers\n",
        "df_nouveau = df.withColumn('playoffs_int', when(df.playoffs==True, 1).otherwise(0))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_nouveau.show()\n",
        "```\n",
        "\n",
        "La nouvelle colonne `playoffs_int` affiche maintenant toutes les valeurs true et false de la colonne `playoffs` soit comme 1, soit comme 0.\n",
        "\n",
        "Nous pouvons utiliser la fonction `dtypes` pour afficher le type de données de chaque colonne dans ce nouveau DataFrame et vérifier que la nouvelle colonne est effectivement une colonne d'entiers :\n",
        "\n",
        "```python\n",
        "# Afficher le type de données de chaque colonne\n",
        "df_nouveau.dtypes\n",
        "```\n",
        "\n",
        "Nous pouvons voir que la nouvelle colonne `playoffs_int` est effectivement une colonne d'entiers."
      ],
      "metadata": {
        "id": "MJ8HybWR-ZQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rq0piVuk-y5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bivariables"
      ],
      "metadata": {
        "id": "MZzcvtPSI2Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment créer une matrice de corrélation"
      ],
      "metadata": {
        "id": "x7LBnPxPBS6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment créer une matrice de corrélation\n",
        "\n",
        "Une matrice de corrélation est une table carrée qui montre les coefficients de corrélation entre les variables dans un ensemble de données.\n",
        "\n",
        "Elle offre un moyen rapide de comprendre la force des relations linéaires qui existent entre les variables dans un ensemble de données.\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour créer une matrice de corrélation à partir d'un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Convertir chaque colonne du DataFrame en vecteurs\n",
        "colonne_vecteur = 'corr_features'\n",
        "assembleur = VectorAssembler(inputCols=df.columns, outputCol=colonne_vecteur)\n",
        "df_vecteur = assembleur.transform(df).select(colonne_vecteur)\n",
        "\n",
        "# Calculer la matrice de corrélation\n",
        "matrice_corr = Correlation.corr(df_vecteur, colonne_vecteur)\n",
        "\n",
        "# Afficher la matrice de corrélation\n",
        "matrice_corr.collect()[0]['pearson({})'.format(colonne_vecteur)].values\n",
        "```\n",
        "\n",
        "Ce code utilise la fonction VectorAssembler pour convertir d'abord les colonnes du DataFrame en vecteurs, puis utilise la fonction Correlation de pyspark.ml.stat pour calculer la matrice de corrélation.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment créer une matrice de corrélation en PySpark**\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les passes, les rebonds et les points pour divers joueurs de basket-ball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [[4, 12, 22],\n",
        "           [5, 14, 24],\n",
        "           [5, 13, 26],\n",
        "           [6, 7, 26],\n",
        "           [7, 8, 29],\n",
        "           [8, 8, 32],\n",
        "           [8, 9, 20],\n",
        "           [10, 13, 14]]\n",
        "  \n",
        "# Définir les noms des colonnes\n",
        "colonnes = ['passes', 'rebonds', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour créer une matrice de corrélation pour ce DataFrame :\n",
        "\n",
        "```python\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Convertir chaque colonne du DataFrame en vecteurs\n",
        "colonne_vecteur = 'corr_features'\n",
        "assembleur = VectorAssembler(inputCols=df.columns, outputCol=colonne_vecteur)\n",
        "df_vecteur = assembleur.transform(df).select(colonne_vecteur)\n",
        "\n",
        "# Calculer la matrice de corrélation\n",
        "matrice_corr = Correlation.corr(df_vecteur, colonne_vecteur)\n",
        "\n",
        "# Afficher la matrice de corrélation\n",
        "matrice_corr.collect()[0]['pearson({})'.format(colonne_vecteur)].values\n",
        "```\n",
        "\n",
        "Les coefficients de corrélation le long de la diagonale du tableau sont tous égaux à 1 car chaque variable est parfaitement corrélée avec elle-même.\n",
        "\n",
        "Tous les autres coefficients de corrélation indiquent la corrélation entre différentes combinaisons de paires de variables.\n",
        "\n",
        "Par exemple :\n",
        "\n",
        "- Le coefficient de corrélation entre les passes et les rebonds est -0,245.\n",
        "- Le coefficient de corrélation entre les passes et les points est -0,330.\n",
        "- Le coefficient de corrélation entre les rebonds et les points est -0,522."
      ],
      "metadata": {
        "id": "5mK4gI8V9nV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Calculer la Corrélation Entre Deux Colonnes"
      ],
      "metadata": {
        "id": "gt4NX2qrCSXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Calculer la Corrélation Entre Deux Colonnes\n",
        "\n",
        "Pour calculer le coefficient de corrélation entre deux colonnes dans un DataFrame PySpark, vous pouvez utiliser la syntaxe suivante :\n",
        "\n",
        "```python\n",
        "df.stat.corr('colonne1', 'colonne2')\n",
        "```\n",
        "\n",
        "Ce code particulier renverra une valeur entre -1 et 1 qui représente le coefficient de corrélation de Pearson entre colonne1 et colonne2.\n",
        "\n",
        "**Exemple**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les passes décisives, les rebonds et les points pour différents joueurs de basket :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [[4, 12, 22],\n",
        "           [5, 14, 24],\n",
        "           [5, 13, 26],\n",
        "           [6, 7, 26],\n",
        "           [7, 8, 29],\n",
        "           [8, 8, 32],\n",
        "           [8, 9, 20],\n",
        "           [10, 13, 14]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['passes_decisives', 'rebonds', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+----------------+--------+------+\n",
        "|passes_decisives| rebonds|points|\n",
        "+----------------+--------+------+\n",
        "|               4|      12|    22|\n",
        "|               5|      14|    24|\n",
        "|               5|      13|    26|\n",
        "|               6|       7|    26|\n",
        "|               7|       8|    29|\n",
        "|               8|       8|    32|\n",
        "|               8|       9|    20|\n",
        "|              10|      13|    14|\n",
        "+----------------+--------+------+\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour calculer la corrélation entre les colonnes passes_decisives et points dans le DataFrame :\n",
        "\n",
        "```python\n",
        "# Calculer la corrélation entre les colonnes passes_decisives et points\n",
        "df.stat.corr('passes_decisives', 'points')\n",
        "```\n",
        "\n",
        "Résultat :\n",
        "\n",
        "```\n",
        "-0.32957304910500873\n",
        "```\n",
        "\n",
        "Le coefficient de corrélation s'avère être -0.32957.\n",
        "\n",
        "Comme cette valeur est négative, cela nous indique qu'il existe une association négative entre les deux variables.\n",
        "\n",
        "En d'autres termes, lorsque la valeur des passes décisives augmente, la valeur des points tend à diminuer.\n",
        "\n",
        "Et lorsque la valeur des passes décisives diminue, la valeur des points tend à augmenter.\n",
        "\n",
        "N'hésitez pas à remplacer passes_decisives et points par les noms de colonnes de votre choix pour calculer le coefficient de corrélation entre deux colonnes différentes."
      ],
      "metadata": {
        "id": "cf-3RXcU3KmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ajouter de données"
      ],
      "metadata": {
        "id": "FpqxnnFsGtbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Ajouter des Jours à une Colonne de Date"
      ],
      "metadata": {
        "id": "M87x0fd0B4BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Ajouter des Jours à une Colonne de Date\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour ajouter un nombre spécifique de jours à une colonne de date dans un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Ajouter 5 jours à chaque date dans la colonne 'date'\n",
        "df.withColumn('date_plus_5', F.date_add(df['date'], 5)).show()\n",
        "```\n",
        "\n",
        "Cet exemple particulier crée une nouvelle colonne appelée date_plus_5 qui ajoute 5 jours à chaque date de la colonne date.\n",
        "\n",
        "**Exemple**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les ventes effectuées à différentes dates dans une entreprise :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['2023-01-15', 225],\n",
        "           ['2023-02-24', 260],\n",
        "           ['2023-07-14', 413],\n",
        "           ['2023-10-30', 368],\n",
        "           ['2023-11-03', 322],\n",
        "           ['2023-11-26', 278]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['date', 'sales']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+----------+-----+\n",
        "|      date|sales|\n",
        "+----------+-----+\n",
        "|2023-01-15|  225|\n",
        "|2023-02-24|  260|\n",
        "|2023-07-14|  413|\n",
        "|2023-10-30|  368|\n",
        "|2023-11-03|  322|\n",
        "|2023-11-26|  278|\n",
        "+----------+-----+\n",
        "```\n",
        "\n",
        "Supposons que nous voulions ajouter une nouvelle colonne qui ajoute 5 jours à chaque date dans la colonne date.\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour le faire :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Ajouter 5 jours à chaque date dans la colonne 'date'\n",
        "df.withColumn('date_plus_5', F.date_add(df['date'], 5)).show()\n",
        "```\n",
        "\n",
        "```\n",
        "+----------+-----+-----------+\n",
        "|      date|sales|date_plus_5|\n",
        "+----------+-----+-----------+\n",
        "|2023-01-15|  225| 2023-01-20|\n",
        "|2023-02-24|  260| 2023-03-01|\n",
        "|2023-07-14|  413| 2023-07-19|\n",
        "|2023-10-30|  368| 2023-11-04|\n",
        "|2023-11-03|  322| 2023-11-08|\n",
        "|2023-11-26|  278| 2023-12-01|\n",
        "+----------+-----+-----------+\n",
        "```\n",
        "\n",
        "Remarquez que la nouvelle colonne date_plus_5 contient chacune des dates de la colonne date avec cinq jours ajoutés.\n",
        "\n",
        "Notez que si vous préférez soustraire 5 jours, vous pouvez utiliser la fonction date_sub() à la place :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Soustraire 5 jours à chaque date dans la colonne 'date'\n",
        "df.withColumn('date_sub_5', F.date_sub(df['date'], 5)).show()\n",
        "```\n",
        "\n",
        "```\n",
        "+----------+-----+----------+\n",
        "|      date|sales|date_sub_5|\n",
        "+----------+-----+----------+\n",
        "|2023-01-15|  225|2023-01-10|\n",
        "|2023-02-24|  260|2023-02-19|\n",
        "|2023-07-14|  413|2023-07-09|\n",
        "|2023-10-30|  368|2023-10-25|\n",
        "|2023-11-03|  322|2023-10-29|\n",
        "|2023-11-26|  278|2023-11-21|\n",
        "+----------+-----+----------+\n",
        "```\n",
        "\n",
        "Remarquez que la nouvelle colonne date_sub_5 contient chacune des dates de la colonne date avec cinq jours soustraits.\n",
        "\n",
        "Notez que nous avons utilisé la fonction withColumn pour retourner un nouveau DataFrame avec la colonne date_sub_5 ajoutée et toutes les autres colonnes"
      ],
      "metadata": {
        "id": "jVEzaCgK6yhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aRpe1TV3G2V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Ajouter Plusieurs Colonnes à un DataFrame"
      ],
      "metadata": {
        "id": "4kMASzTfBn1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Ajouter Plusieurs Colonnes à un DataFrame\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes pour ajouter plusieurs nouvelles colonnes à un DataFrame PySpark :\n",
        "\n",
        "**Méthode 1 : Ajouter Plusieurs Colonnes Vides**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Ajouter trois colonnes vides\n",
        "for col in ['new_col1', 'new_col2', 'new_col3']:\n",
        "    df = df.withColumn(col, lit(None))\n",
        "```\n",
        "\n",
        "**Méthode 2 : Ajouter Plusieurs Colonnes Basées sur les Colonnes Existantes**\n",
        "\n",
        "```python\n",
        "# Ajouter trois nouvelles colonnes basées sur les valeurs dans la colonne 'points'\n",
        "df = df.withColumn('points2', df.points*2)\\\n",
        "       .withColumn('points3', df.points*3)\\\n",
        "       .withColumn('points_half', df.points/2)\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Arrière', 11],\n",
        "           ['A', None, 8],\n",
        "           ['A', 'Avant', 22],\n",
        "           ['A', 'Avant', 22],\n",
        "           ['B', 'Arrière', 14],\n",
        "           ['B', 'Arrière', 14],\n",
        "           ['B', 'Avant', 13],\n",
        "           ['B', 'Avant', 7]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'position', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+------+--------+------+\n",
        "|equipe|position|points|\n",
        "+------+--------+------+\n",
        "|     A|  Arrière|    11|\n",
        "|     A|    null|     8|\n",
        "|     A|   Avant|    22|\n",
        "|     A|   Avant|    22|\n",
        "|     B|  Arrière|    14|\n",
        "|     B|  Arrière|    14|\n",
        "|     B|   Avant|    13|\n",
        "|     B|   Avant|     7|\n",
        "+------+--------+------+\n",
        "```\n",
        "\n",
        "**Exemple 1 : Ajouter Plusieurs Colonnes Vides**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour ajouter trois nouvelles colonnes vides au DataFrame existant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Ajouter trois colonnes vides\n",
        "for col in ['new_col1', 'new_col2', 'new_col3']:\n",
        "    df = df.withColumn(col, lit(None))\n",
        "\n",
        "# Afficher le DataFrame mis à jour\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+------+--------+------+--------+--------+--------+\n",
        "|equipe|position|points|new_col1|new_col2|new_col3|\n",
        "+------+--------+------+--------+--------+--------+\n",
        "|     A|  Arrière|    11|    null|    null|    null|\n",
        "|     A|    null|     8|    null|    null|    null|\n",
        "|     A|   Avant|    22|    null|    null|    null|\n",
        "|     A|   Avant|    22|    null|    null|    null|\n",
        "|     B|  Arrière|    14|    null|    null|    null|\n",
        "|     B|  Arrière|    14|    null|    null|    null|\n",
        "|     B|   Avant|    13|    null|    null|    null|\n",
        "|     B|   Avant|     7|    null|    null|    null|\n",
        "+------+--------+------+--------+--------+--------+\n",
        "```\n",
        "\n",
        "Remarquez que trois nouvelles colonnes avec les noms new_col1, new_col2 et new_col3 ont toutes été ajoutées au DataFrame existant.\n",
        "\n",
        "Comme nous utilisons la fonction lit pour spécifier une valeur littérale de None, les valeurs dans chaque nouvelle colonne sont simplement toutes des valeurs nulles.\n",
        "\n",
        "**Exemple 2 : Ajouter Plusieurs Colonnes Basées sur les Colonnes Existantes**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour ajouter trois nouvelles colonnes dont les valeurs sont toutes basées sur les valeurs existantes dans la colonne points du DataFrame :\n",
        "\n",
        "```python\n",
        "# Ajouter trois nouvelles colonnes basées sur les valeurs dans la colonne 'points'\n",
        "df = df.withColumn('points2', df.points*2)\\\n",
        "       .withColumn('points3', df.points*3)\\\n",
        "       .withColumn('points_half', df.points/2)\n",
        "\n",
        "# Afficher le DataFrame mis à jour\n",
        "df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "+------+--------+------+-------+-------+-----------+\n",
        "|equipe|position|points|points2|points3|points_half|\n",
        "+------+--------+------+-------+-------+-----------+\n",
        "|     A|  Arrière|    11|     22|     33|        5.5|\n",
        "|     A|    null|     8|     16|     24|        4.0|\n",
        "|     A|   Avant|    22|     44|     66|       11.0|\n",
        "|     A|   Avant|    22|     44|     66|       11.0|\n",
        "|     B|  Arrière|    14|     28|     42|        7.0|\n",
        "|     B|  Arrière|    14|     28|     42|        7.0|\n",
        "|     B|   Avant|    13|     26|     39|        6.5|\n",
        "|\n",
        "\n",
        "     B|   Avant|     7|     14|     21|        3.5|\n",
        "+------+--------+------+-------+-------+-----------+\n",
        "```\n",
        "\n",
        "Remarquez que trois nouvelles colonnes ont été ajoutées au DataFrame dont les valeurs sont basées sur les valeurs existantes dans la colonne points."
      ],
      "metadata": {
        "id": "zf09kiY28AFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtrage de données"
      ],
      "metadata": {
        "id": "pvotZ7oMFcrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Filtrer les Lignes en Utilisant NOT LIKE"
      ],
      "metadata": {
        "id": "ubPikn3nF3u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Filtrer les Lignes en Utilisant NOT LIKE\n",
        "\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour filtrer un DataFrame PySpark en utilisant un opérateur NOT LIKE :\n",
        "\n",
        "```python\n",
        "df.filter(~df.team.like('%avs%')).show()\n",
        "```\n",
        "\n",
        "Cet exemple particulier filtre le DataFrame pour afficher uniquement les lignes où la chaîne dans la colonne de l'équipe ne contient pas un motif comme \"avs\" quelque part dans la chaîne.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment Filtrer en Utilisant NOT LIKE en PySpark**\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les points marqués par divers joueurs de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['Mavs', 18],\n",
        "           ['Nets', 33],\n",
        "           ['Lakers', 12],\n",
        "           ['Mavs', 15],\n",
        "           ['Cavs', 19],\n",
        "           ['Wizards', 24],\n",
        "           ['Cavs', 28],\n",
        "           ['Nets', 40],\n",
        "           ['Mavs', 24],\n",
        "           ['Spurs', 13]]\n",
        "  \n",
        "# Définir les noms des colonnes\n",
        "colonnes = ['equipe', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne contenir que les lignes où la colonne de l'équipe ne contient pas un motif comme \"avs\" quelque part dans la chaîne :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où la colonne de l'équipe ne contient pas un motif comme 'avs'\n",
        "df.filter(~df.team.like('%avs%')).show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame résultant ne contient pas un motif comme \"avs\" dans la colonne de l'équipe.\n",
        "\n",
        "Notez que nous avons utilisé la fonction like pour trouver toutes les chaînes dans la colonne de l'équipe qui avaient un motif comme \"avs\" et ensuite nous avons utilisé le symbole ~ pour nier cette fonction.\n",
        "\n",
        "Le résultat final est que nous pouvons filtrer uniquement les lignes dans le DataFrame qui n'ont pas de motif comme \"avs\" dans la colonne de l'équipe."
      ],
      "metadata": {
        "id": "3gSQfALgFfmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Filtrer pour \"Ne Contient Pas\""
      ],
      "metadata": {
        "id": "Xe4ZtCavD-m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Filtrer pour \"Ne Contient Pas\"\n",
        "\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour filtrer un DataFrame PySpark en utilisant un opérateur \"Ne Contient Pas\" :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où l'équipe ne contient pas 'avs'\n",
        "df.filter(~df.team.contains('avs')).show()\n",
        "```\n",
        "\n",
        "**Exemple**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur les points marqués par divers joueurs de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['Mavs', 14],\n",
        "           ['Nets', 22],\n",
        "           ['Nets', 31],\n",
        "           ['Cavs', 27],\n",
        "           ['Kings', 26],\n",
        "           ['Spurs', 40],\n",
        "           ['Lakers', 23],\n",
        "           ['Spurs', 17],]\n",
        "  \n",
        "# Définir les noms des colonnes\n",
        "colonnes = ['equipe', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame afin de ne contenir que les lignes où la colonne de l'équipe ne contient pas \"avs\" n'importe où dans la chaîne :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où l'équipe ne contient pas 'avs'\n",
        "df.filter(~df.team.contains('avs')).show()\n",
        "```\n",
        "\n",
        "Remarquez que aucune des lignes dans le DataFrame résultant ne contient \"avs\" dans la colonne de l'équipe.\n",
        "\n",
        "Notez que les lignes contenant Mavs et Cavs dans la colonne de l'équipe ont toutes deux été filtrées car ces deux équipes contenaient \"avs\" dans leur nom."
      ],
      "metadata": {
        "id": "WV-0Hiu2D--S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s0YwO5xoGvqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Filtrer par Plage de Dates"
      ],
      "metadata": {
        "id": "1N6cMSuEK_Tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Filtrer par Plage de Dates\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour filtrer les lignes dans un DataFrame PySpark en fonction d'une plage de dates :\n",
        "\n",
        "```python\n",
        "# spécifier les dates de début et de fin\n",
        "dates = ('2019-01-01', '2022-01-01')\n",
        "\n",
        "# filtrer le DataFrame pour afficher uniquement les lignes entre les dates de début et de fin\n",
        "df.filter(df.start_date.between(*dates)).show()\n",
        "```\n",
        "\n",
        "Cet exemple particulier filtre le DataFrame pour ne contenir que les lignes où la date dans la colonne start_date du DataFrame est entre le 1er janvier 2019 et le 1er janvier 2022.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment Filtrer par Plage de Dates en PySpark**\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur la date de début de divers employés dans une entreprise :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', '2017-10-25'],\n",
        "           ['B', '2018-10-11'],\n",
        "           ['C', '2018-10-17'],\n",
        "           ['D', '2019-12-21'],\n",
        "           ['E', '2021-04-14'],\n",
        "           ['F', '2022-06-26']]\n",
        "  \n",
        "# Définir les noms des colonnes\n",
        "colonnes = ['employé', 'date_de_début']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne contenir que les lignes où la date dans la colonne start_date du DataFrame est entre le 1er janvier 2019 et le 1er janvier 2022 :\n",
        "\n",
        "```python\n",
        "# spécifier les dates de début et de fin\n",
        "dates = ('2019-01-01', '2022-01-01')\n",
        "\n",
        "# filtrer le DataFrame pour afficher uniquement les lignes entre les dates de début et de fin\n",
        "df.filter(df.start_date.between(*dates)).show()\n",
        "```\n",
        "\n",
        "Remarquez que le DataFrame a été filtré pour afficher uniquement les lignes avec les deux dates dans la colonne start_date qui tombent entre le 1er janvier 2019 et le 1er janvier 2022.\n",
        "\n",
        "Notez que si vous voulez seulement savoir combien de lignes ont une date dans une plage de dates spécifique, alors vous pouvez utiliser la fonction count comme suit :\n",
        "\n",
        "```python\n",
        "# spécifier les dates de début et de fin\n",
        "dates = ('2019-01-01', '2022-01-01')\n",
        "\n",
        "# compter le nombre de lignes dans le DataFrame qui tombent entre les dates de début et de fin\n",
        "df.filter(df.start_date.between(*dates)).count()\n",
        "```\n",
        "\n",
        "Cela nous indique qu'il y a deux lignes dans le DataFrame où la date dans la colonne start_date tombe entre le 1er janvier 2019 et le 1er janvier 2022."
      ],
      "metadata": {
        "id": "hZ59q-wpK5fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrer par Colonne Booléenne en PySpark"
      ],
      "metadata": {
        "id": "rwSiRTTHLTt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtrer par Colonne Booléenne\n",
        "\n",
        "Vous pouvez utiliser les méthodes suivantes pour filtrer les lignes d'un DataFrame PySpark en fonction des valeurs dans une colonne booléenne :\n",
        "\n",
        "**Méthode 1 : Filtrer en Fonction des Valeurs dans une Seule Colonne Booléenne**\n",
        "\n",
        "```python\n",
        "# filtrer pour les lignes où la valeur dans la colonne 'all_star' est True\n",
        "df.filter(df.all_star==True).show()\n",
        "```\n",
        "\n",
        "**Méthode 2 : Filtrer en Fonction des Valeurs dans Plusieurs Colonnes Booléennes**\n",
        "\n",
        "```python\n",
        "# filtrer pour les lignes où les valeurs dans les colonnes 'all_star' et 'starter' sont toutes deux True\n",
        "df.filter((df.all_star==True) & (df.starter==True)).show()\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant qui contient des informations sur divers joueurs de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 18, True, False],\n",
        "           ['B', 20, False, True],\n",
        "           ['C', 25, True, True],\n",
        "           ['D', 40, True, True],\n",
        "           ['E', 34, True, False],\n",
        "           ['F', 32, False, False],\n",
        "           ['G', 19, False, False]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'points', 'all_star', 'starter']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Filtrer en Fonction des Valeurs dans une Seule Colonne Booléenne**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne contenir que les lignes où la valeur dans la colonne all_star est vraie :\n",
        "\n",
        "```python\n",
        "# filtrer pour les lignes où la valeur dans la colonne 'all_star' est True\n",
        "df.filter(df.all_star==True).show()\n",
        "```\n",
        "\n",
        "**Exemple 2 : Filtrer en Fonction des Valeurs dans Plusieurs Colonnes Booléennes**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne contenir que les lignes où la valeur dans la colonne all_star est vraie et la valeur dans la colonne starter est vraie :\n",
        "\n",
        "```python\n",
        "# filtrer pour les lignes où les valeurs dans les colonnes 'all_star' et 'starter' sont toutes deux True\n",
        "df.filter((df.all_star==True) & (df.starter==True)).show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame filtré a une valeur True dans la colonne all_star et une valeur True dans la colonne starter."
      ],
      "metadata": {
        "id": "UDIPkdspLQSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Utiliser \"IS NOT IN\""
      ],
      "metadata": {
        "id": "-Jz0U0z_LtKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Utiliser \"IS NOT IN\"\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante en PySpark pour filtrer les lignes d'un DataFrame où une valeur dans une colonne particulière n'est pas dans une liste particulière :\n",
        "\n",
        "**Définir un tableau de valeurs**\n",
        "```python\n",
        "my_array = ['A', 'D', 'E']\n",
        "```\n",
        "\n",
        "**Filtrer le DataFrame pour ne contenir que les lignes où 'team' n'est pas dans my_array**\n",
        "```python\n",
        "df.filter(~df.team.isin(my_array)).show()\n",
        "```\n",
        "\n",
        "Cet exemple particulier filtrera le DataFrame pour ne contenir que les lignes où la valeur dans la colonne team n'est pas égale à A, D ou E.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment Utiliser \"IS NOT IN\" en PySpark**\n",
        "\n",
        "Supposons que nous avons le DataFrame PySpark suivant qui contient des informations sur divers joueurs de basketball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Est', 11, 4],\n",
        "           ['A', 'Est', 8, 9],\n",
        "           ['A', 'Est', 10, 3],\n",
        "           ['B', 'Ouest', 6, 12],\n",
        "           ['B', 'Ouest', 6, 4],\n",
        "           ['C', 'Est', 5, 2],\n",
        "           ['D', 'Est', 14, 2],\n",
        "           ['E', 'Ouest', 25, 2]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'conference', 'points', 'assists']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne montrer que les lignes où la valeur dans la colonne team n'est pas égale à A, D ou E :\n",
        "\n",
        "**Définir un tableau de valeurs**\n",
        "```python\n",
        "my_array = ['A', 'D', 'E']\n",
        "```\n",
        "\n",
        "**Filtrer le DataFrame pour ne contenir que les lignes où 'team' n'est pas dans my_array**\n",
        "```python\n",
        "df.filter(~df.team.isin(my_array)).show()\n",
        "```\n",
        "\n",
        "Le DataFrame résultant ne contient que les lignes où la valeur dans la colonne team n'est pas égale à A, D ou E.\n",
        "\n",
        "Note : L'opérateur tilde ( ~ ) est utilisé en PySpark pour représenter NON.\n",
        "\n",
        "En utilisant cet opérateur avec la fonction isin, nous pouvons filtrer le DataFrame pour ne contenir que les lignes où la valeur dans une colonne particulière n'est pas dans une liste de valeurs."
      ],
      "metadata": {
        "id": "fpZng4PHLh3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Utiliser l'Opérateur \"Différent de\""
      ],
      "metadata": {
        "id": "ravyrUs_MF0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Utiliser l'Opérateur \"Différent de\"\n",
        "\n",
        "Il existe deux façons courantes de filtrer un DataFrame PySpark en utilisant un opérateur \"Différent de\" :\n",
        "\n",
        "**Méthode 1 : Filtrer en Utilisant un Opérateur \"Différent de\"**\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où l'équipe n'est pas égale à 'A'\n",
        "df.filter(df.team!='A').show()\n",
        "```\n",
        "\n",
        "**Méthode 2 : Filtrer en Utilisant Plusieurs Opérateurs \"Différent de\"**\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où l'équipe n'est pas égale à 'A' et les points ne sont pas égaux à 5\n",
        "df.filter((df.team!='A') & (df.points!=5)).show()\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Est', 11, 4],\n",
        "           ['A', 'Est', 8, 9],\n",
        "           ['A', 'Est', 10, 3],\n",
        "           ['B', 'Ouest', 6, 12],\n",
        "           ['B', 'Ouest', 6, 4],\n",
        "           ['C', 'Est', 5, 2]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'conference', 'points', 'assists']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Filtrer en Utilisant un Opérateur \"Différent de\"**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne contenir que les lignes où la colonne team n'est pas égale à A :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où l'équipe n'est pas égale à 'A'\n",
        "df.filter(df.team!='A').show()\n",
        "```\n",
        "\n",
        "Le DataFrame résultant ne contient que les lignes où la valeur dans la colonne team n'est pas égale à A.\n",
        "\n",
        "**Exemple 2 : Filtrer en Utilisant Plusieurs Opérateurs \"Différent de\"**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour ne contenir que les lignes où la colonne team n'est pas égale à A et la valeur dans la colonne points n'est pas égale à 5 :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où l'équipe n'est pas égale à 'A' et les points ne sont pas égaux à 5\n",
        "df.filter((df.team!='A') & (df.points!=5)).show()\n",
        "```\n",
        "\n",
        "Le DataFrame résultant ne contient que les lignes où la valeur dans la colonne team n'est pas égale à A et la valeur dans la colonne points n'est pas égale à 5."
      ],
      "metadata": {
        "id": "e5iTu4PVMG6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrer les Lignes Basées sur les Valeurs dans une Liste en PySpark"
      ],
      "metadata": {
        "id": "5VF0NLrERC-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtrer les Lignes Basées sur les Valeurs dans une Liste en PySpark\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour filtrer un DataFrame PySpark pour les lignes contenant une valeur d'une liste spécifique :\n",
        "\n",
        "```python\n",
        "# spécifier les valeurs à filtrer\n",
        "ma_liste = ['Mavs', 'Kings', 'Spurs']\n",
        "\n",
        "# filtrer les lignes où l'équipe est dans la liste\n",
        "df.filter(df.team.isin(ma_liste)).show()\n",
        "```\n",
        "\n",
        "Cet exemple particulier filtre le DataFrame pour ne contenir que les lignes où la valeur dans la colonne team est égale à l'une des valeurs de la liste que nous avons spécifiée.\n",
        "\n",
        "L'exemple suivant montre comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Comment Filtrer les Lignes Basées sur les Valeurs dans une Liste en PySpark**\n",
        "\n",
        "Supposons que nous avons le DataFrame PySpark suivant qui contient des informations sur les points marqués par différents joueurs de basket-ball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['Mavs', 18],\n",
        "           ['Nets', 33],\n",
        "           ['Lakers', 12],\n",
        "           ['Mavs', 15],\n",
        "           ['Kings', 19],\n",
        "           ['Wizards', 24],\n",
        "           ['Magic', 28],\n",
        "           ['Nets', 40],\n",
        "           ['Mavs', 24],\n",
        "           ['Spurs', 13]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['equipe', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour filtrer le DataFrame pour les lignes où la colonne team est égale au nom d'une équipe dans une liste spécifique :\n",
        "\n",
        "```python\n",
        "# spécifier les valeurs à filtrer\n",
        "ma_liste = ['Mavs', 'Kings', 'Spurs']\n",
        "\n",
        "# filtrer les lignes où l'équipe est dans la liste\n",
        "df.filter(df.team.isin(ma_liste)).show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame filtré a une valeur d'équipe égale à Mavs, Kings ou Spurs, qui sont les trois noms d'équipes que nous avons spécifiés dans notre liste."
      ],
      "metadata": {
        "id": "rFNO8V4kRFVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation de l'opérateur \"OU\""
      ],
      "metadata": {
        "id": "C5DFv1qLRGz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilisation de l'opérateur \"OU\"\n",
        "\n",
        "Il existe deux façons courantes de filtrer un DataFrame PySpark en utilisant un opérateur \"OU\" :\n",
        "\n",
        "**Méthode 1 : Utiliser \"OU\"**\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 9 ou team est égal à \"B\"\n",
        "df.filter('points>9 or team==\"B\"').show()\n",
        "```\n",
        "\n",
        "**Méthode 2 : Utiliser le symbole |**\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 9 ou team est égal à \"B\"\n",
        "df.filter((df.points>9) | (df.team==\"B\")).show()\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'East', 11, 4],\n",
        "           ['A', 'East', 8, 9],\n",
        "           ['A', 'East', 10, 3],\n",
        "           ['B', 'West', 6, 12],\n",
        "           ['B', 'West', 6, 4],\n",
        "           ['C', 'East', 5, 2]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['team', 'conference', 'points', 'assists']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Filtrer le DataFrame en Utilisant \"OU\"**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante avec la fonction de filtrage et le mot-clé or pour filtrer le DataFrame pour ne contenir que les lignes où la valeur dans la colonne points est supérieure à 9 ou la valeur dans la colonne team est égale à B :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 9 ou team est égal à \"B\"\n",
        "df.filter('points>9 or team==\"B\"').show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame résultant satisfait au moins l'une des conditions suivantes :\n",
        "\n",
        "- La valeur dans la colonne points est supérieure à 9\n",
        "- La valeur dans la colonne team est égale à \"B\"\n",
        "\n",
        "**Exemple 2 : Filtrer le DataFrame en Utilisant le Symbole |**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante avec la fonction de filtrage et le symbole | pour filtrer le DataFrame pour ne contenir que les lignes où la valeur dans la colonne points est supérieure à 9 ou la valeur dans la colonne team est égale à B :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 9 ou team est égal à \"B\"\n",
        "df.filter((df.points>9) | (df.team==\"B\")).show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame résultant satisfait au moins l'une des conditions suivantes :\n",
        "\n",
        "- La valeur dans la colonne points est supérieure à 9\n",
        "- La valeur dans la colonne team est égale à \"B\"\n",
        "\n",
        "Remarquez également que ce DataFrame correspond au DataFrame de l'exemple précédent."
      ],
      "metadata": {
        "id": "AJdBP4W0RHET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation de l'opérateur \"ET\""
      ],
      "metadata": {
        "id": "UHVXDScBRHms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilisation de l'opérateur \"ET\"\n",
        "\n",
        "\n",
        "Il existe deux façons courantes de filtrer un DataFrame PySpark en utilisant un opérateur \"ET\" :\n",
        "\n",
        "**Méthode 1 : Utiliser \"ET\"**\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 5 et conference est égal à \"East\"\n",
        "df.filter('points>5 and conference==\"East\"').show()\n",
        "```\n",
        "\n",
        "**Méthode 2 : Utiliser le symbole &**\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 5 et conference est égal à \"East\"\n",
        "df.filter((df.points>5) & (df.conference==\"East\")).show()\n",
        "```\n",
        "\n",
        "Les exemples suivants montrent comment utiliser chaque méthode en pratique avec le DataFrame PySpark suivant :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'East', 11, 4],\n",
        "           ['A', 'East', 8, 9],\n",
        "           ['A', 'East', 10, 3],\n",
        "           ['B', 'West', 6, 12],\n",
        "           ['B', 'West', 6, 4],\n",
        "           ['C', 'East', 5, 2]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['team', 'conference', 'points', 'assists']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**Exemple 1 : Filtrer le DataFrame en Utilisant \"ET\"**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante avec la fonction de filtrage et le mot-clé and pour filtrer le DataFrame pour ne contenir que les lignes où la valeur dans la colonne points est supérieure à 5 et la valeur dans la colonne conference est égale à \"East\" :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 5 et conference est égal à \"East\"\n",
        "df.filter('points>5 and conference==\"East\"').show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame résultant satisfait à la fois les conditions suivantes :\n",
        "\n",
        "- La valeur dans la colonne points est supérieure à 5\n",
        "- La valeur dans la colonne conference est égale à \"East\"\n",
        "\n",
        "**Exemple 2 : Filtrer le DataFrame en Utilisant le Symbole &**\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante avec la fonction de filtrage et le symbole & pour filtrer le DataFrame pour ne contenir que les lignes où la valeur dans la colonne points est supérieure à 5 et la valeur dans la colonne conference est égale à \"East\" :\n",
        "\n",
        "```python\n",
        "# Filtrer le DataFrame où points est supérieur à 5 et conference est égal à \"East\"\n",
        "df.filter((df.points>5) & (df.conference==\"East\")).show()\n",
        "```\n",
        "\n",
        "Remarquez que chacune des lignes dans le DataFrame résultant satisfait à la fois les conditions suivantes :\n",
        "\n",
        "- La valeur dans la colonne points est supérieure à 5\n",
        "- La valeur dans la colonne conference est égale à \"East\""
      ],
      "metadata": {
        "id": "aqIZ50iaRHyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace"
      ],
      "metadata": {
        "id": "k_W11yZdRH_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment remplacer une chaîne dans une colonne"
      ],
      "metadata": {
        "id": "-dc8Lvr7RICL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment remplacer une chaîne dans une colonne avec PySpark\n",
        "*PAR ZACH BOBBITT PUBLIÉ LE 16 OCTOBRE 2023*\n",
        "\n",
        "Vous pouvez utiliser la syntaxe suivante pour remplacer une chaîne spécifique dans une colonne d'un DataFrame PySpark :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Remplacer 'Guard' par 'Gd' dans la colonne position\n",
        "df_nouveau = df.withColumn('position', regexp_replace('position', 'Guard', 'Gd'))\n",
        "```\n",
        "\n",
        "Cet exemple particulier remplace la chaîne \"Guard\" par la nouvelle chaîne \"Gd\" dans la colonne position du DataFrame.\n",
        "\n",
        "Les exemples suivants montrent comment utiliser cette syntaxe en pratique.\n",
        "\n",
        "**Exemple : Remplacer une chaîne dans une colonne d'un DataFrame PySpark**\n",
        "\n",
        "Supposons que nous ayons le DataFrame PySpark suivant qui contient des informations sur divers joueurs de basket-ball :\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Définir les données\n",
        "donnees = [['A', 'Guard', 11],\n",
        "           ['A', 'Guard', 8],\n",
        "           ['A', 'Forward', 22],\n",
        "           ['A', 'Forward', 22],\n",
        "           ['B', 'Guard', 14],\n",
        "           ['B', 'Guard', 14],\n",
        "           ['B', 'Guard', 13],\n",
        "           ['B', 'Forward', 7],\n",
        "           ['C', 'Guard', 8],\n",
        "           ['C', 'Forward', 5]]\n",
        "  \n",
        "# Définir les noms de colonnes\n",
        "colonnes = ['team', 'position', 'points']\n",
        "  \n",
        "# Créer le DataFrame en utilisant les données et les noms de colonnes\n",
        "df = spark.createDataFrame(donnees, colonnes)\n",
        "  \n",
        "# Afficher le DataFrame\n",
        "df.show()\n",
        "```\n",
        "\n",
        "Nous pouvons utiliser la syntaxe suivante pour remplacer la chaîne \"Guard\" par la nouvelle chaîne \"Gd\" dans la colonne position du DataFrame :\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Remplacer 'Guard' par 'Gd' dans la colonne position\n",
        "df_nouveau = df.withColumn('position', regexp_replace('position', 'Guard', 'Gd'))\n",
        "\n",
        "# Afficher le nouveau DataFrame\n",
        "df_nouveau.show()\n",
        "```\n",
        "\n",
        "Dans la sortie, nous pouvons voir que chaque occurrence de \"Guard\" a été remplacée par \"Gd\" dans la colonne position du DataFrame."
      ],
      "metadata": {
        "id": "9o7-H9w7RIEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "exE80fr-RIHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8HMsfkbuR3pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "from sympy import Polyhedron, Halfspace\n",
        "\n",
        "# Supposons que nous avons les coordonnées des pixels et leurs valeurs de gris pour une image MNIST\n",
        "# Ici, nous utiliserons des valeurs de pixels arbitraires à des fins d'illustration\n",
        "pixel_coords = [(1, 1), (2, 2), (3, 1), (4, 3)]  # Coordonnées des pixels (x, y)\n",
        "pixel_values = [0.1, 0.2, 0.15, 0.3]  # Valeurs de gris des pixels\n",
        "\n",
        "# Création des variables symboliques pour les coordonnées et les valeurs de gris\n",
        "x, y, value = sp.symbols('x y value')\n",
        "\n",
        "# Création des demi-espaces pour modéliser les inégalités linéaires\n",
        "halfspaces = [\n",
        "    Halfspace(x + y, -3),\n",
        "    Halfspace(2*x - y, -1),\n",
        "    Halfspace(-value, 0.1),\n",
        "    Halfspace(value, 0.3)\n",
        "]\n",
        "\n",
        "# Création d'un polyèdre à partir des demi-espaces\n",
        "polyhedron = Polyhedron(halfspaces)\n",
        "\n",
        "# Affichage du polyèdre résultant\n",
        "print(polyhedron)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "QyLxeAtUxvmZ",
        "outputId": "f7051029-4f2b-47c3-aba3-5f1e2c3a0111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Polyhedron' from 'sympy' (/usr/local/lib/python3.10/dist-packages/sympy/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3a7d0a7202b6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolyhedron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHalfspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Supposons que nous avons les coordonnées des pixels et leurs valeurs de gris pour une image MNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ici, nous utiliserons des valeurs de pixels arbitraires à des fins d'illustration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Polyhedron' from 'sympy' (/usr/local/lib/python3.10/dist-packages/sympy/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce script, j'utilise scipy.spatial.ConvexHull pour créer un polyèdre convexe à partir des sommets définis. Ensuite, je sélectionne aléatoirement un point à l'intérieur du polyèdre pour contrôler les paramètres de bruit à appliquer à l'image MNIST. Vous pouvez ajuster les sommets du polyèdre selon vos besoins pour contrôler les niveaux et les types de bruit ajoutés aux images."
      ],
      "metadata": {
        "id": "TKdQGHGE2oEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce script, le polyèdre est utilisé pour contrôler les paramètres de bruit qui seront appliqués à l'image MNIST. Plus précisément :\n",
        "\n",
        "1. **Définition des sommets du polyèdre :** Les sommets du polyèdre sont définis comme des points dans un espace de paramètres. Dans cet exemple, les sommets sont définis par les coordonnées `[0.1, 0.1]`, `[0.5, 0.5]`, et `[0.9, 0.1]`. Ces coordonnées représentent des combinaisons de paramètres de bruit qui seront utilisées pour générer des images bruitées.\n",
        "\n",
        "2. **Création du polyèdre convexe :** À l'aide de la fonction `ConvexHull` de `scipy.spatial`, un polyèdre convexe est créé à partir des sommets définis. Cela garantit que tout point à l'intérieur du polyèdre peut être représenté comme une combinaison convexe des sommets.\n",
        "\n",
        "3. **Sélection aléatoire d'un point dans le polyèdre :** Un point est sélectionné aléatoirement à l'intérieur du polyèdre à l'aide de la distribution de Dirichlet. Ce point représente les paramètres de bruit spécifiques qui seront utilisés pour générer l'image bruitée.\n",
        "\n",
        "4. **Génération de l'image bruitée :** En fonction du point sélectionné dans le polyèdre, les paramètres de bruit correspondants sont obtenus en interpolant entre les sommets du polyèdre. Ensuite, du bruit gaussien est ajouté à l'image MNIST en utilisant ces paramètres de bruit pour générer l'image bruitée finale.\n",
        "\n",
        "En ajustant les sommets du polyèdre, vous pouvez contrôler les niveaux et les types de bruit qui seront ajoutés aux images MNIST générées."
      ],
      "metadata": {
        "id": "DEG8GF2Q2eJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normaliser les valeurs de pixels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "# Fonction pour ajouter du bruit gaussien à une image\n",
        "def add_gaussian_noise(image, mean=0, std=0.1):\n",
        "    noise = np.random.normal(mean, std, image.shape)\n",
        "    noisy_image = np.clip(image + noise, 0, 1)\n",
        "    return noisy_image\n",
        "\n",
        "# Définir les sommets du polyèdre pour contrôler le bruit\n",
        "polyhedron_vertices = np.array([[0.1, 0.1], [0.5, 0.5], [0.9, 0.1]])\n",
        "\n",
        "# Créer un polyèdre convexe à partir des sommets\n",
        "hull = ConvexHull(polyhedron_vertices)\n",
        "\n",
        "# Fonction pour générer une image bruitée en interpolant entre les sommets du polyèdre\n",
        "def generate_noisy_image(image, polyhedron_point):\n",
        "    # Interpolation pour obtenir les paramètres de bruit correspondants\n",
        "    noise_params = np.dot(polyhedron_point, polyhedron_vertices[hull.vertices])\n",
        "\n",
        "    # Ajout de bruit gaussien à l'image\n",
        "    noisy_image = add_gaussian_noise(image, mean=noise_params[0], std=noise_params[1])\n",
        "\n",
        "    return noisy_image\n",
        "\n",
        "# Sélection aléatoire d'un point dans le polyèdre\n",
        "polyhedron_point = np.random.dirichlet(np.ones(len(polyhedron_vertices)))\n",
        "\n",
        "# Sélection aléatoire d'une image MNIST\n",
        "index = np.random.randint(0, len(x_train))\n",
        "image = x_train[index]\n",
        "\n",
        "# Génération de l'image bruitée\n",
        "noisy_image = generate_noisy_image(image, polyhedron_point)\n",
        "\n",
        "# Affichage des images\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title('Image originale')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(noisy_image, cmap='gray')\n",
        "plt.title('Image bruitée')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "mvWuSBnl0zCj",
        "outputId": "01f754f1-d735-414f-d036-d9e79bf2054a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normaliser les valeurs de pixels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "# Sélection aléatoire d'une image MNIST\n",
        "index = np.random.randint(0, len(x_train))\n",
        "image = x_train[index]\n",
        "\n",
        "# Définir une variation delta pour les sommets du polyèdre\n",
        "delta = 0.2\n",
        "\n",
        "# Créer les sommets du polyèdre en fonction de l'image originale\n",
        "image_flattened = image.flatten()\n",
        "polyhedron_vertices = np.array([image_flattened - delta, image_flattened + delta])\n",
        "\n",
        "# Créer un polyèdre convexe à partir des sommets\n",
        "hull = ConvexHull(polyhedron_vertices.T)\n",
        "\n",
        "# Sélection aléatoire d'un point dans le polyèdre\n",
        "polyhedron_point = np.random.dirichlet(np.ones(len(hull.vertices)), size=1)  # Modifier la taille en 1\n",
        "\n",
        "# Fonction pour générer une image bruitée en interpolant entre les sommets du polyèdre\n",
        "def generate_noisy_image(image, polyhedron_point):\n",
        "    # Interpolation pour obtenir les paramètres de bruit correspondants\n",
        "    noise_params = np.dot(polyhedron_point, polyhedron_vertices)\n",
        "\n",
        "    # Remise en forme des paramètres de bruit\n",
        "    noise_params = noise_params.reshape(-1)  # Modifier la forme en un vecteur\n",
        "\n",
        "    # Interpolation linéaire entre les sommets du polyèdre\n",
        "    noise_params = noise_params[0] + polyhedron_point[0][:, np.newaxis] * (noise_params[1] - noise_params[0])\n",
        "\n",
        "    # Remettre l'image dans sa forme originale\n",
        "    image_shape = image.shape\n",
        "    noise_params = noise_params.reshape(image_shape)\n",
        "\n",
        "    # Ajout de bruit gaussien à l'image\n",
        "    noisy_image = np.clip(image + noise_params, 0, 1)\n",
        "\n",
        "    return noisy_image\n",
        "\n",
        "# Génération de l'image bruitée\n",
        "noisy_image = generate_noisy_image(image, polyhedron_point)\n",
        "\n",
        "# Affichage des images\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title('Image originale')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(noisy_image, cmap='gray')\n",
        "plt.title('Image bruitée')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "-IfAQs6a5D_0",
        "outputId": "ca4d7c5d-41b1-48d9-f710-70191399f20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (1,6) and (2,784) not aligned: 6 (dim 1) != 2 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5a1bedee8e09>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Génération de l'image bruitée\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mnoisy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_noisy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyhedron_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Affichage des images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5a1bedee8e09>\u001b[0m in \u001b[0;36mgenerate_noisy_image\u001b[0;34m(image, polyhedron_point)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_noisy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyhedron_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Interpolation pour obtenir les paramètres de bruit correspondants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnoise_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolyhedron_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyhedron_vertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Remise en forme des paramètres de bruit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1,6) and (2,784) not aligned: 6 (dim 1) != 2 (dim 0)"
          ]
        }
      ]
    }
  ]
}